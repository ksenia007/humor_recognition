{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>meanGrade</th>\n",
       "      <th>score_std</th>\n",
       "      <th>basic_score</th>\n",
       "      <th>scaled_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump cites fake boob to make the case that su...</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.687184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Louisiana school district : All students must ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Green receives standing ovation at ‘ The Colo...</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Judge Orders State Department To Provide Withh...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATT Loses Another 1.36 Million Pay TV Subscrib...</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  meanGrade  score_std  \\\n",
       "0  Trump cites fake boob to make the case that su...   1.166667   0.687184   \n",
       "1  Louisiana school district : All students must ...   1.000000   0.632456   \n",
       "2   Green receives standing ovation at ‘ The Colo...   1.600000   1.200000   \n",
       "3  Judge Orders State Department To Provide Withh...   1.000000   0.632456   \n",
       "4  ATT Loses Another 1.36 Million Pay TV Subscrib...   1.400000   0.800000   \n",
       "\n",
       "   basic_score  scaled_mean  \n",
       "0          0.0     0.388889  \n",
       "1          0.0     0.333333  \n",
       "2          1.0     0.533333  \n",
       "3          0.0     0.333333  \n",
       "4          0.0     0.466667  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/created_datasets/humicroedit_unpaired_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21990, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trump cites fake boob to make the case that support for impeachment is falling'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = data.loc[0].text\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e27ca550fbe45d8944e550ddf88052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pretrained_for_sentence(sent, len_sent = 25):\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    padded_tokens = tokens + ['[PAD]' for _ in range(len_sent - len(tokens))]\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    sent_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    \n",
    "    #Step 5: Get BERT vocabulary index for each token\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    #Converting everything to torch tensors before feeding them to bert_model\n",
    "    token_ids = torch.tensor(token_ids).unsqueeze(0) \n",
    "    attn_mask = torch.tensor(attn_mask).unsqueeze(0) \n",
    "    #Feed them to bert\n",
    "    hidden_reps, cls_head = bert_model(token_ids, attention_mask = attn_mask)\n",
    "    \n",
    "    return hidden_reps, cls_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.5 ms ± 5.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_pretrained_for_sentence(\"Test this function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually training model\n",
    "\n",
    "Note: followed this tutorial https://medium.com/swlh/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>meanGrade</th>\n",
       "      <th>score_std</th>\n",
       "      <th>basic_score</th>\n",
       "      <th>scaled_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump cites fake boob to make the case that su...</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.687184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Louisiana school district : All students must ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Green receives standing ovation at ‘ The Colo...</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Judge Orders State Department To Provide Withh...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATT Loses Another 1.36 Million Pay TV Subscrib...</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  meanGrade  score_std  \\\n",
       "0  Trump cites fake boob to make the case that su...   1.166667   0.687184   \n",
       "1  Louisiana school district : All students must ...   1.000000   0.632456   \n",
       "2   Green receives standing ovation at ‘ The Colo...   1.600000   1.200000   \n",
       "3  Judge Orders State Department To Provide Withh...   1.000000   0.632456   \n",
       "4  ATT Loses Another 1.36 Million Pay TV Subscrib...   1.400000   0.800000   \n",
       "\n",
       "   basic_score  scaled_mean  \n",
       "0          0.0     0.388889  \n",
       "1          0.0     0.333333  \n",
       "2          1.0     0.533333  \n",
       "3          0.0     0.333333  \n",
       "4          0.0     0.466667  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, maxlen):\n",
    "        self.df = pd.read_csv(filename)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #Initialize the BERT tokenizer\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'text']\n",
    "        label = self.df.loc[index, 'scaled_mean']\n",
    "        \n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        #Tokenize the sentence\n",
    "        tokens_orig = self.tokenizer.tokenize(sentence) \n",
    "        \n",
    "        tokens = ['[CLS]'] + tokens_orig + ['[SEP]']\n",
    "        \n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating instances of training and validation set\n",
    "train_set = HumorDataset(filename = 'data/created_datasets/humicroedit_unpaired_train.csv', maxlen = 30)\n",
    "val_set = HumorDataset(filename = 'data/created_datasets/humicroedit_unpaired_valid.csv', maxlen = 30)\n",
    "#Creating intsances of training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size = 64)\n",
    "val_loader = DataLoader(val_set, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  101,  8398, 17248,  8275, 22017,  2497,  2000,  2191,  1996,  2553,\n",
      "         2008,  2490,  2005, 17727,  5243, 22729,  2003,  4634,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]), 0.3888888888888889)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "class HumorRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_bert = True):\n",
    "        super(HumorRegressor, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        #Freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        #Regression layer\n",
    "        self.fc1 = nn.Linear(768, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
    "\n",
    "        #Obtaining the representation of [CLS] head\n",
    "        out = cont_reps[:, 0]\n",
    "        \n",
    "        #Feeding cls_rep to the regressor layer\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        #preds = self.cls_layer(cls_rep)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = HumorRegressor(freeze_bert = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(2):\n",
    "    loss_cum=0\n",
    "    for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "        #Clear gradients\n",
    "        opti.zero_grad()  \n",
    "        #Converting these to cuda tensors\n",
    "        #seq, attn_masks, labels = seq.cuda(args.gpu), attn_masks.cuda(args.gpu), labels.cuda(args.gpu)\n",
    "\n",
    "        #Obtaining the logits from the model\n",
    "        preds = net(seq, attn_masks)\n",
    "\n",
    "        #Computing loss\n",
    "        loss = criterion(preds.squeeze(-1), labels.float())\n",
    "        loss_cum += loss\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if (it + 1) % 5 == 0:\n",
    "            print(\"Iteration {} of epoch {} complete. Loss : {}\".format(it+1, ep+1, \n",
    "                                                                    loss_cum.item()))\n",
    "            loss_cum = 0\n",
    "    loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for it, (seq, attn_masks, labels) in enumerate(val_loader):\n",
    "            preds = net(seq, attn_masks)\n",
    "            loss = criterion(preds.squeeze(-1), labels.float())\n",
    "            loss_val += loss\n",
    "            if (it + 1) % 10 == 0: print('Iteration ', it)\n",
    "        print('MSE on validaiton: ', loss_val)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = 0\n",
    "with torch.no_grad():\n",
    "    for it, (seq, attn_masks, labels) in enumerate(val_loader):\n",
    "        preds = net(seq, attn_masks)\n",
    "        loss = criterion(preds.squeeze(-1), labels.float())\n",
    "        loss_val += loss\n",
    "        if (it + 1) % 10 == 0: print('Iteration ', it)\n",
    "print('MSE on validaiton: ', loss_val)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at examples from the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>original</th>\n",
       "      <th>edit</th>\n",
       "      <th>grades</th>\n",
       "      <th>meanGrade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6000</td>\n",
       "      <td>3986</td>\n",
       "      <td>Trump Replacing Secretary of &lt;State/&gt; Tillerso...</td>\n",
       "      <td>Class</td>\n",
       "      <td>11000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6001</td>\n",
       "      <td>9504</td>\n",
       "      <td>When George W. Bush &lt;stood/&gt; with Hillary Clinton</td>\n",
       "      <td>knitted</td>\n",
       "      <td>11111</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6002</td>\n",
       "      <td>13642</td>\n",
       "      <td>South Korea &lt;hospital/&gt; fire : dozens feared d...</td>\n",
       "      <td>camp</td>\n",
       "      <td>21000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6003</td>\n",
       "      <td>9371</td>\n",
       "      <td>&lt;Trump/&gt; predicts Patriots will win Super Bow...</td>\n",
       "      <td>gypsy</td>\n",
       "      <td>33200</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6004</td>\n",
       "      <td>2947</td>\n",
       "      <td>Sessions announces new conditions for sanctuar...</td>\n",
       "      <td>launder</td>\n",
       "      <td>33210</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                           original  \\\n",
       "0        6000   3986  Trump Replacing Secretary of <State/> Tillerso...   \n",
       "1        6001   9504  When George W. Bush <stood/> with Hillary Clinton   \n",
       "2        6002  13642  South Korea <hospital/> fire : dozens feared d...   \n",
       "3        6003   9371   <Trump/> predicts Patriots will win Super Bow...   \n",
       "4        6004   2947  Sessions announces new conditions for sanctuar...   \n",
       "\n",
       "      edit  grades  meanGrade  \n",
       "0    Class   11000        0.4  \n",
       "1  knitted   11111        1.0  \n",
       "2     camp   21000        0.6  \n",
       "3    gypsy   33200        1.6  \n",
       "4  launder   33210        1.8  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data = pd.read_csv('data/task-1/val_split.csv')\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_sentence(sentence, replacement, maxlen):\n",
    "    sentence_change = replace_word(sentence, replacement)\n",
    "    sentence = drop_replacement_symbols(sentence)\n",
    "\n",
    "    #Preprocessing the text to be suitable for BERT\n",
    "    tokens_orig = tokenizer.tokenize(sentence) #Tokenize the sentence\n",
    "    tokens_new = tokenizer.tokenize(sentence_change)\n",
    "    tokens = ['[CLS]'] + tokens_orig + ['[SEP]'] + tokens_new + ['[SEP]'] \n",
    "    if len(tokens) < maxlen:\n",
    "        tokens = tokens + ['[PAD]' for _ in range(maxlen - len(tokens))] #Padding sentences\n",
    "    else:\n",
    "        tokens = tokens[:maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "    tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "    #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "    attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "    return tokens_ids_tensor, attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Trump Replacing Secretary of <State/> Tillerson With CIA Director Mike Pompeo : NPR\n",
      "Alternative:  Trump Replacing Secretary of Class Tillerson With CIA Director Mike Pompeo : NPR\n",
      "Prediction is  tensor(0.8303)  True value:  0.4\n",
      "Sentence:  When George W. Bush <stood/> with Hillary Clinton\n",
      "Alternative:  When George W. Bush knitted with Hillary Clinton\n",
      "Prediction is  tensor(1.1504)  True value:  1.0\n",
      "Sentence:  South Korea <hospital/> fire : dozens feared dead and many injured\n",
      "Alternative:  South Korea camp fire : dozens feared dead and many injured\n",
      "Prediction is  tensor(0.7940)  True value:  0.6\n",
      "Sentence:   <Trump/> predicts Patriots will win Super Bowl by 8 points\n",
      "Alternative:   gypsy predicts Patriots will win Super Bowl by 8 points\n",
      "Prediction is  tensor(0.8824)  True value:  1.6\n",
      "Sentence:  Sessions announces new conditions for sanctuary cities to <get/> federal money\n",
      "Alternative:  Sessions announces new conditions for sanctuary cities to launder federal money\n",
      "Prediction is  tensor(1.2976)  True value:  1.8\n",
      "Sentence:  Trump to <unveil/> punishing trade actions against China Thursday\n",
      "Alternative:  Trump to cancel punishing trade actions against China Thursday\n",
      "Prediction is  tensor(0.9215)  True value:  0.6\n",
      "Sentence:  FBI nominee says Trump-Russia <probe/> is no ' witch hunt '\n",
      "Alternative:  FBI nominee says Trump-Russia infatuation is no ' witch hunt '\n",
      "Prediction is  tensor(1.0354)  True value:  0.6\n",
      "Sentence:  Former Trump <campaign/> adviser : Info given to Russian spies ' immaterial '\n",
      "Alternative:  Former Trump sandwich adviser : Info given to Russian spies ' immaterial '\n",
      "Prediction is  tensor(1.1732)  True value:  0.8\n",
      "Sentence:  NRA ’s Wayne LaPierre instructs CPAC to “ be frightened ” of “ <socialist/> wave ” following Parkland\n",
      "Alternative:  NRA ’s Wayne LaPierre instructs CPAC to “ be frightened ” of “ tidal wave ” following Parkland\n",
      "Prediction is  tensor(0.2724)  True value:  1.0\n",
      "Sentence:  Catholic priest caught driving 13-year-old girl to motel after paying 16-year-old <pimp/> \n",
      "Alternative:  Catholic priest caught driving 13-year-old girl to motel after paying 16-year-old cat \n",
      "Prediction is  tensor(1.1587)  True value:  1.2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    (ids, attn) = run_for_sentence(valid_data.loc[i].original, valid_data.loc[i].edit, maxlen=50)\n",
    "    with torch.no_grad():\n",
    "        pred = net(ids.unsqueeze(0), attn.unsqueeze(0))\n",
    "    print('Sentence: ', valid_data.loc[i].original)\n",
    "    print('Alternative: ', replace_word(valid_data.loc[i].original, valid_data.loc[i].edit))\n",
    "    print('Prediction is ', pred[0][0]*3, ' True value: ', valid_data.loc[i].meanGrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsea-disease",
   "language": "python",
   "name": "deepsea-disease"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
